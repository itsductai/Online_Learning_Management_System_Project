{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3895371",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "## GHI CHÃš Tá»”NG QUAN (NOTE â€“ DÃ€NH CHO KAGGLE)\n",
    "# =============================================\n",
    " **Má»¥c tiÃªu**\n",
    " - Fine-tune mÃ´ hÃ¬nh T5 (seq2seq) cho bÃ i toÃ¡n Text-to-SQL trÃªn Kaggle.\n",
    " - DÃ¹ng dataset cÃ´ng khai (b-mc2/sql-create-context, Clinton/Text-to-sql-v1, knowrohit07/know_sql),\n",
    " sau Ä‘Ã³ trá»™n, tiá»n xá»­ lÃ½ (tokenize), huáº¥n luyá»‡n, lÆ°u mÃ´ hÃ¬nh.\n",
    " - KhÃ´ng thay Ä‘á»•i logic cá»‘t lÃµi; chá»‰ bá»• sung chÃº thÃ­ch chi tiáº¿t.\n",
    "\n",
    " **Luá»“ng chÃ­nh**\n",
    " 1) CÃ i Ä‘áº·t thÆ° viá»‡n â†’ Import.\n",
    " 2) Náº¡p/ghÃ©p dataset cÃ³ sáºµn â†’ lÆ°u cache (merged_dataset).\n",
    " 3) Tiá»n xá»­ lÃ½ (tokenize) â†’ lÆ°u cache (tokenized_datasets).\n",
    " 4) Kiá»ƒm thá»­ zero-shot (baseline) trÃªn model gá»‘c.\n",
    " 5) Huáº¥n luyá»‡n full fine-tune â†’ lÆ°u mÃ´ hÃ¬nh Ä‘Ã£ fine-tune.\n",
    " 6) Kiá»ƒm thá»­ láº¡i vá»›i mÃ´ hÃ¬nh fine-tuned + Ä‘Ã¡nh giÃ¡ (ROUGE) máº«u nhá».\n",
    " 7) ÄÃ³ng gÃ³i mÃ´ hÃ¬nh (zip) Ä‘á»ƒ táº£i xuá»‘ng / dÃ¹ng vá» sau.\n",
    "\n",
    " **Äáº§u ra (sáº£n pháº©m)**\n",
    " - ThÆ° má»¥c model Ä‘Ã£ fine-tune: /kaggle/working/sql_t5_finetuned\n",
    " - CÃ³ thá»ƒ náº¡p vá» app (FastAPI/Flask/.NET-bridge) báº±ng from_pretrained(\"/kaggle/working/sql_t5_finetuned\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41687c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.0.0rc2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.3.1)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyarrow-22.0.0\n"
     ]
    }
   ],
   "source": [
    "# CÃ i Ä‘áº·t thÆ° viá»‡n datasets náº¿u chÆ°a cÃ³\n",
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8df26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.0.0rc2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub>=0.7.0->evaluate) (8.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.3.1)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n"
     ]
    }
   ],
   "source": [
    "# CÃ i Ä‘áº·t thÆ° viá»‡n evaluate\n",
    "!pip install evaluate \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b263d40",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 2) IMPORT CÃC THÆ¯ VIá»†N Cáº¦N THIáº¾T\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0018ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124 True\n"
     ]
    }
   ],
   "source": [
    "import torch; \n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63540d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 10:33:45.611153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762079625.772075      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762079625.822072      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_dataset, interleave_datasets, load_from_disk # Dataset Huggingface\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer # Model vÃ  Tokenizer cho trainning\n",
    "import torch # ThÆ° viá»‡n PyTorch tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½ GPU, tensor\n",
    "import time # Ghi thá»i gian trainning\n",
    "import evaluate # ThÆ° viá»‡n evaluate Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh (BLEU, ROUGE)\n",
    "import pandas as pd # ThÆ° viá»‡n pandas Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u dáº¡ng báº£ng/káº¿t quáº£\n",
    "import numpy as np # ThÆ° viá»‡n numpy Ä‘á»ƒ xá»­ lÃ½ máº£ng\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Bá» qua cÃ¡c cáº£nh bÃ¡o khÃ´ng cáº§n thiáº¿t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654dd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52e01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (1.0.0rc2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 1.0.0rc2\n",
      "    Uninstalling huggingface-hub-1.0.0rc2:\n",
      "      Successfully uninstalled huggingface-hub-1.0.0rc2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.36.0\n"
     ]
    }
   ],
   "source": [
    "# Import thÆ° viá»‡n transformers vÃ  huggingface_hub\n",
    "!pip install transformers huggingface_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration # Model vÃ  Tokenizer T5\n",
    "\n",
    "import os # ThÆ° viá»‡n os Ä‘á»ƒ thao tÃ¡c vá»›i há»‡ thá»‘ng tá»‡p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b364701",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 3) Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN VÃ€ THIáº¾T Bá»Š\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4826fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_model_path = '/kaggle/working/' # ÄÆ°á»ng dáº«n lÆ°u model trÃªn Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3181124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.is_available() # Kiá»ƒm tra GPU cÃ³ sáºµn khÃ´ng, tráº£ vá» True hoáº·c False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e2f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6558add5c610409fa289eeb25374390b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086422a04b7c49bc92c8404ef669c772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b6816866244d929ac634e352cfbed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b03fe9f5674f05bdf149a8fe7a4aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_merged_quantized.onnx:   0%|          | 0.00/58.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76f694d7e794849931dbcea0f00df3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_model_quantized.onnx:   0%|          | 0.00/58.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fcec15cc834151be0e5b339512519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_with_past_model.onnx:   0%|          | 0.00/220M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d5c3e2664747e985daf8d22cd92edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/decoder_with_past_model_quantized.o(â€¦):   0%|          | 0.00/55.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c9e4f09617478daad0667ab1f68e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/encoder_model.onnx:   0%|          | 0.00/141M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96471362674e47d9bfffe0568000e720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/encoder_model_quantized.onnx:   0%|          | 0.00/35.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf7c7ca16c1485fa7fe152b54211fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dff6393d884f4199612d557a40c8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rust_model.ot:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e045707cc83344afb92491a446fb6fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721ff41c7ce74e60a046cc5022ae5d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4758e4afa734b6a8817b2f4a5ae58b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394b964dfe3349e8ad6000ac84cea5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Táº£i snapshot repo model vá» LOCAL FOLDER (chá»‰ gá»i API 1 láº§n, khÃ´ng Ä‘á»¥ng 'additional_chat_templates')\n",
    "#   - repo_id 't5-small' sáº½ redirect vá» 'google-t5/t5-small'\n",
    "local_dir = snapshot_download(repo_id=\"t5-small\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name='t5-small' # TÃªn model ná»n; code gá»‘c chá»n t5-small cho baseline/finetune\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir, local_files_only=True)\n",
    "\n",
    "# TrÃªn Kaggle GPU cÃ³ thá»ƒ dÃ¹ng bfloat16, cÃ²n local CPU thÃ¬ Ä‘á»ƒ máº·c Ä‘á»‹nh FP32\n",
    "if device == \"cuda\":\n",
    "    original_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        model_name, torch_dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "else:\n",
    "    original_model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are using the default legacy behaviour\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924d1f9",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 4 ) Náº P/CHUáº¨N Bá»Š DATASET (CÃ“ CACHE)\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e0ffc",
   "metadata": {},
   "source": [
    "# ==== Load OLSM from RAW JSON/JSONL vÃ  clean data ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1accf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OLSM] Wrote 22988 cleaned examples -> /kaggle/working/olsm-clean.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e9cb8265a84511b4725f37cd880084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'context', 'answer'],\n",
       "     num_rows: 18390\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'context', 'answer'],\n",
       "     num_rows: 2299\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'context', 'answer'],\n",
       "     num_rows: 2299\n",
       " }))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== ONE-CELL: Load OLSM from RAW JSON/JSONL (tá»± sá»­a & split) ====\n",
    "from datasets import load_dataset\n",
    "import json, re, os, codecs\n",
    "\n",
    "RAW_FILES = [\n",
    "    \"/kaggle/input/olsm-data/dataset-olsm.json\",\n",
    "    \"/kaggle/input/olsm-data/dataset-olsm2.json\",\n",
    "    \"/kaggle/input/dataset-basic/dataset-basic.json\",\n",
    "    \"/kaggle/input/dataset-basic/dataset-noizy.json\",\n",
    "]\n",
    "OUT = \"/kaggle/working/olsm-clean.jsonl\"\n",
    "os.makedirs(os.path.dirname(OUT), exist_ok=True)\n",
    "\n",
    "# thay â€œsmart quotesâ€ -> ASCII\n",
    "SMART = str.maketrans({\n",
    "    \"â€œ\": '\"', \"â€\": '\"', \"â€\": '\"', \"Â«\": '\"', \"Â»\": '\"',\n",
    "    \"â€˜\": \"'\", \"â€™\": \"'\", \"â€š\": \"'\", \"â€¹\": \"'\", \"â€º\": \"'\",\n",
    "})\n",
    "\n",
    "def _fix_inner_quotes(text: str, field: str) -> str:\n",
    "    # Äá»•i dáº¥u \" bÃªn trong giÃ¡ trá»‹ field -> ' vÃ  escape \\n,\\t\n",
    "    pat = re.compile(rf'\"{field}\"\\s*:\\s*\"(.*?)\"', re.DOTALL)\n",
    "    def repl(m):\n",
    "        v = m.group(1)\n",
    "        v = v.replace('\\\\\"', '\"')        # bá» escape cÅ© lá»™n xá»™n\n",
    "        v = v.replace('\"', \"'\")          # \" -> '\n",
    "        v = v.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "        v = v.replace(\"\\n\",\"\\\\n\").replace(\"\\t\",\"\\\\t\")\n",
    "        return f'\"{field}\":\"{v}\"'\n",
    "    return pat.sub(repl, text)\n",
    "\n",
    "def _objects_from_text(txt: str):\n",
    "    # náº¿u lÃ  JSON array chuáº©n -> parse má»™t láº§n\n",
    "    t = txt.strip()\n",
    "    if t.startswith(\"[\") and t.endswith(\"]\"):\n",
    "        try:\n",
    "            arr = json.loads(t)\n",
    "            for obj in arr:\n",
    "                yield obj\n",
    "            return\n",
    "        except Exception:\n",
    "            pass\n",
    "    # cÃ²n láº¡i coi nhÆ° JSONL \"láº«n lá»™n\": tÃ¡ch cÃ¡c object dÃ­nh liá»n }{\n",
    "    txt = re.sub(r\"}\\s*{\", \"}\\n{\", txt)\n",
    "    for line in txt.splitlines():\n",
    "        L = line.strip().rstrip(\",\")\n",
    "        if not L:\n",
    "            continue\n",
    "        try:\n",
    "            yield json.loads(L)\n",
    "        except Exception:\n",
    "            # cá»‘ gáº¯ng trÃ­ch Ä‘oáº¡n {...} lá»›n nháº¥t rá»“i sá»­a\n",
    "            s_idx, e_idx = L.find(\"{\"), L.rfind(\"}\")\n",
    "            if s_idx == -1 or e_idx == -1 or e_idx <= s_idx:\n",
    "                continue\n",
    "            seg = L[s_idx:e_idx+1]\n",
    "            for f in (\"question\",\"context\",\"answer\"):\n",
    "                seg = _fix_inner_quotes(seg, f)\n",
    "            seg = re.sub(r\",\\s*}\", \"}\", seg)\n",
    "            try:\n",
    "                yield json.loads(seg)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "# Gá»™p & lÃ m sáº¡ch hai file vÃ o má»™t JSONL\n",
    "kept = 0\n",
    "with open(OUT, \"w\", encoding=\"utf-8\") as w:\n",
    "    for path in RAW_FILES:\n",
    "        with open(path, \"rb\") as f:\n",
    "            txt = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        # bá» BOM + smart quotes\n",
    "        bom = codecs.BOM_UTF8.decode(\"utf-8\")\n",
    "        if txt.startswith(bom): txt = txt.lstrip(bom)\n",
    "        txt = txt.translate(SMART)\n",
    "\n",
    "        for obj in _objects_from_text(txt):\n",
    "            if not all(k in obj for k in (\"question\",\"context\",\"answer\")):\n",
    "                continue\n",
    "            q = str(obj[\"question\"]).strip()\n",
    "            c = str(obj[\"context\"]).strip()\n",
    "            a = str(obj[\"answer\"]).strip()\n",
    "            if not (q and c and a):\n",
    "                continue\n",
    "            w.write(json.dumps({\"question\": q, \"context\": c, \"answer\": a}, ensure_ascii=False) + \"\\n\")\n",
    "            kept += 1\n",
    "\n",
    "print(f\"[OLSM] Wrote {kept} cleaned examples -> {OUT}\")\n",
    "\n",
    "# Load 1 láº§n tá»« file sáº¡ch vÃ  split 80/10/10\n",
    "olsm_full  = load_dataset(\"json\", data_files=OUT, split=\"train\")\n",
    "tmp        = olsm_full.train_test_split(test_size=0.2, seed=42)\n",
    "olsm_train = tmp[\"train\"]\n",
    "tmp2       = tmp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "olsm_test, olsm_val = tmp2[\"train\"], tmp2[\"test\"]\n",
    "\n",
    "olsm_train, olsm_test, olsm_val  # Ä‘á»ƒ báº¡n tháº¥y kÃ­ch thÆ°á»›c ba split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54928e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created zip: /kaggle/working/olsm-clean.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "src = \"/kaggle/working/olsm-clean.jsonl\"\n",
    "dst = \"/kaggle/working/olsm-clean.zip\"\n",
    "\n",
    "# nÃ©n file JSONL thÃ nh zip\n",
    "shutil.make_archive(dst.replace(\".zip\", \"\"), 'zip', root_dir=\"/kaggle/working\", base_dir=\"olsm-clean.jsonl\")\n",
    "print(\"âœ… Created zip:\", dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba87acc",
   "metadata": {},
   "source": [
    "# **XÃ“A CACHE CÅ¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No old merged_dataset found, will rebuild from scratch\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "\n",
    "# ThÆ° má»¥c cache cÅ©\n",
    "CACHE_DIR = \"merged_dataset\"\n",
    "\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "    print(f\"ğŸ—‘ï¸ Deleted old cache folder: {CACHE_DIR}\")\n",
    "else:\n",
    "    print(\"âœ… No old merged_dataset found, will rebuild from scratch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLSM splits: 18390 2299 2299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9317abd7a63842f3b1a33c902c9274fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ce96ea2022473ba4aa47a7f7e04373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sql_create_context_v4.json:   0%|          | 0.00/21.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70cfff519ed4d6d8871e96297b9a6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/78577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658490bdcf21466f89da33eb90973807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/118 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9614e31ebb1648dab6ca44435559f5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "texttosqlv2.jsonl:   0%|          | 0.00/635M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f4f7dd8d0048868459e776f5a8d388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/262208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a926639dc949189d28df4ae5c9bd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11051055991246cc8bc1cab39487b358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "know_sql_val3{ign}.json:   0%|          | 0.00/13.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fadb361385040a09276f731bab3fa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/49456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0decd698a7184c899108285cee8cfd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/118695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f108ec8ec8aa4e5dabdd9a89a3c72e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14835 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd43d7fe4cb4eb8b24b58d95afdcb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4d305094a24a279f2b8c72b6c4b757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f486f99524ad4a9ea651da9b2d9193a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd68f96b818849619b0c28f288b48590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3304a513bed044a79467cacb09d7ebc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/137085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2323c4b12c4d1396e0466c098ff5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649926ee153c47b9b862c8491c8f6fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013fdc7b6ff1464bbadd9cbdf9ca499a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/137085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept total=124561 | by source: {'OLSM': 16310, 'HF': 108251}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930fd323dabb44369dfbd7828c645b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept total=15001 | by source: {'OLSM': 2197, 'HF': 12804}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7b5eb50c7d492f8a0b8f83c1870405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept total=17058 | by source: {'OLSM': 2223, 'HF': 14835}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1686c91d370409f8f402c84f5d13050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/124561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6530ef1ef0ba465db2fffba938785d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/17058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca15cf0007d4da6b3d52d3fbae753b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged and Saved Dataset âœ…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'context', 'answer'],\n",
       "        num_rows: 124561\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'context', 'answer'],\n",
       "        num_rows: 17058\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'context', 'answer'],\n",
       "        num_rows: 15001\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, DatasetDict, interleave_datasets, concatenate_datasets\n",
    "\n",
    "# Cá»‘ gáº¯ng náº¡p dataset Ä‘Ã£ merge sáºµn tá»« cache \"merged_dataset\" Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian\n",
    "try:\n",
    "    dataset = load_from_disk(\"merged_dataset\")\n",
    "    print(\"Loaded Merged Dataset\")\n",
    "except:\n",
    "    # Náº¿u khÃ´ng cÃ³ cache, tiáº¿n hÃ nh táº£i vÃ  merge cÃ¡c dataset vÃ  chia split thá»§ cÃ´ng\n",
    "\n",
    "    # =========================\n",
    "    # 4.0) Local OLSM datasets\n",
    "    # =========================\n",
    "\n",
    "    olsm_full = load_dataset(\"json\", data_files=\"/kaggle/working/olsm-clean.jsonl\", split=\"train\")\n",
    "    _tmp = olsm_full.train_test_split(test_size=0.2, seed=42)\n",
    "    olsm_train = _tmp[\"train\"]\n",
    "    _tmp2 = _tmp[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    olsm_test, olsm_val = _tmp2[\"train\"], _tmp2[\"test\"]\n",
    "    print(\"OLSM splits:\", olsm_train.num_rows, olsm_val.num_rows, olsm_test.num_rows)\n",
    "    \n",
    "        \n",
    "\n",
    "    # =========================\n",
    "    # 4.1) b-mc2/sql-create-context\n",
    "    # =========================\n",
    "    dataset_scc_train = load_dataset(\"b-mc2/sql-create-context\", split='train[:80%]')       # 80% train\n",
    "    dataset_scc_test  = load_dataset(\"b-mc2/sql-create-context\", split='train[-20%:-10%]')  # 10% test\n",
    "    dataset_scc_val   = load_dataset(\"b-mc2/sql-create-context\", split='train[-10%:]')      # 10% val\n",
    "\n",
    "    # =========================\n",
    "    # 4.2) Clinton/Text-to-sql-v1 (Ä‘á»•i tÃªn cá»™t)\n",
    "    # =========================\n",
    "    dataset_tts_train = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[:80%]')\n",
    "    dataset_tts_train = dataset_tts_train.remove_columns(['source', 'text'])\n",
    "    dataset_tts_train = dataset_tts_train.rename_columns({'instruction':'question','input':'context','response':'answer'})\n",
    "\n",
    "    dataset_tts_test  = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[-20%:-10%]')\n",
    "    dataset_tts_test  = dataset_tts_test.remove_columns(['source', 'text'])\n",
    "    dataset_tts_test  = dataset_tts_test.rename_columns({'instruction':'question','input':'context','response':'answer'})\n",
    "\n",
    "    dataset_tts_val   = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[-10%:]')\n",
    "    dataset_tts_val   = dataset_tts_val.remove_columns(['source', 'text'])\n",
    "    dataset_tts_val   = dataset_tts_val.rename_columns({'instruction':'question','input':'context','response':'answer'})\n",
    "\n",
    "    # =========================\n",
    "    # 4.3) knowrohit07/know_sql\n",
    "    # =========================\n",
    "    dataset_ks_train  = load_dataset(\"knowrohit07/know_sql\", split='validation[:80%]')\n",
    "    dataset_ks_test   = load_dataset(\"knowrohit07/know_sql\", split='validation[-20%:-10%]')\n",
    "    dataset_ks_val    = load_dataset(\"knowrohit07/know_sql\", split='validation[-10%:]')\n",
    "\n",
    "\n",
    "    # 4.4) GhÃ©p 3 nguá»“n láº¡i báº±ng interleave_datasets Ä‘á»ƒ tÄƒng Ä‘a dáº¡ng dá»¯ liá»‡u\n",
    "    # =========================\n",
    "    from datasets import interleave_datasets, concatenate_datasets, DatasetDict\n",
    "\n",
    "    # 4.4.1) Interleave 3 bá»™ HF (máº·c Ä‘á»‹nh first_exhausted -> dá»«ng á»Ÿ bá»™ ngáº¯n nháº¥t Ä‘á»ƒ giá»¯ cÃ¢n báº±ng)\n",
    "    mix_train = interleave_datasets([dataset_scc_train, dataset_tts_train, dataset_ks_train])\n",
    "    mix_test  = interleave_datasets([dataset_scc_test,  dataset_tts_test,  dataset_ks_test])\n",
    "    mix_val   = interleave_datasets([dataset_scc_val,   dataset_tts_val,   dataset_ks_val])\n",
    "\n",
    "    # 4.4.2) Gáº¯n nhÃ£n nguá»“n Ä‘á»ƒ thá»‘ng kÃª\n",
    "    def tag(ds, name):\n",
    "        return ds.map(lambda ex: {\"_src\": name})\n",
    "\n",
    "    mix_train, mix_test, mix_val = tag(mix_train, \"HF\"), tag(mix_test, \"HF\"), tag(mix_val, \"HF\")\n",
    "    olsm_train_t, olsm_test_t, olsm_val_t = tag(olsm_train, \"OLSM\"), tag(olsm_test, \"OLSM\"), tag(olsm_val, \"OLSM\")\n",
    "\n",
    "    # 4.4.3) Concat vá»›i OLSM Äá»¨NG TRÆ¯á»šC Ä‘á»ƒ khi dedup thÃ¬ OLSM Ä‘Æ°á»£c giá»¯ láº¡i náº¿u trÃ¹ng\n",
    "    dataset = DatasetDict({\n",
    "        \"train\":      concatenate_datasets([olsm_train_t, mix_train]).shuffle(seed=42),\n",
    "        \"test\":       concatenate_datasets([olsm_test_t,  mix_test]).shuffle(seed=42),\n",
    "        \"validation\": concatenate_datasets([olsm_val_t,   mix_val]).shuffle(seed=42),\n",
    "    })\n",
    "\n",
    "\n",
    "    # ==== Deduplicate by (question||context||answer) ====\n",
    "    import hashlib\n",
    "    from datasets import DatasetDict\n",
    "\n",
    "    def add_hash(ex):\n",
    "        s = ex[\"question\"] + \"||\" + ex[\"context\"] + \"||\" + ex[\"answer\"]\n",
    "        ex[\"_h\"] = hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "        return ex\n",
    "\n",
    "    dataset = DatasetDict({ k: v.map(add_hash) for k, v in dataset.items() })\n",
    "\n",
    "    def dedup_keep_first(ds):\n",
    "        seen = set()\n",
    "        kept_src = {\"OLSM\": 0, \"HF\": 0}\n",
    "        def f(ex):\n",
    "            h = ex[\"_h\"]\n",
    "            if h in seen:\n",
    "                return False\n",
    "            seen.add(h)\n",
    "            kept_src[ex.get(\"_src\",\"HF\")] = kept_src.get(ex.get(\"_src\",\"HF\"), 0) + 1\n",
    "            return True\n",
    "        out = ds.filter(f)\n",
    "        # in thá»‘ng kÃª nhá»\n",
    "        total = out.num_rows\n",
    "        print(f\"Kept total={total} | by source:\", kept_src)\n",
    "        return out\n",
    "\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        dataset[split] = dedup_keep_first(dataset[split])\n",
    "        dataset[split] = dataset[split].remove_columns([\"_h\",\"_src\"])\n",
    "\n",
    "    dataset.save_to_disk(\"merged_dataset\")\n",
    "    print(\"Merged and Saved Dataset âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "dataset  # Hiá»ƒn thá»‹ info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5553d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['Tell me the 2nd leg for alemannia aachen',\n",
       "  'Season 9 all the titles were no. in series.',\n",
       "  'What is Award, when Category is \"Best Actor\", and when Year is less than 1988?',\n",
       "  'How large was the crowd when North Melbourne played as the away team?',\n",
       "  \"What's the date when Stu Harris was the Runner-up?\",\n",
       "  'Find enrollments for user ID 102 that happened this year.',\n",
       "  'What is the Time of the Player with a Rank of 1 or 2 and Notes of FA?',\n",
       "  'count the number of times patient 15754 was tested for mch in 2104.'],\n",
       " 'context': ['CREATE TABLE table_56562 (\\n    \"Team #1\" text,\\n    \"Agg.\" text,\\n    \"Team #2\" text,\\n    \"1st leg\" text,\\n    \"2nd leg\" text\\n)',\n",
       "  'CREATE TABLE table_3706 (\\n    \"No. in series\" real,\\n    \"Title\" text,\\n    \"Directed by\" text,\\n    \"Written by\" text,\\n    \"Original air date\" text,\\n    \"Production code\" text,\\n    \"U.S. viewers (million)\" text\\n)',\n",
       "  'CREATE TABLE table_name_35 (award VARCHAR, category VARCHAR, year VARCHAR)',\n",
       "  'CREATE TABLE table_name_62 (crowd VARCHAR, away_team VARCHAR)',\n",
       "  'CREATE TABLE table_name_58 (date VARCHAR, runner_up_skip VARCHAR)',\n",
       "  'CREATE TABLE enrollments ( EnrollmentId INT PRIMARY KEY, UserId INT NOT NULL, CourseId INT NOT NULL, CreatedAt DATETIME2 NOT NULL );',\n",
       "  'CREATE TABLE table_name_75 (time VARCHAR, notes VARCHAR, rank VARCHAR)',\n",
       "  'CREATE TABLE procedures_icd (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    icd9_code text,\\n    charttime time\\n)\\n\\nCREATE TABLE diagnoses_icd (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    icd9_code text,\\n    charttime time\\n)\\n\\nCREATE TABLE d_items (\\n    row_id number,\\n    itemid number,\\n    label text,\\n    linksto text\\n)\\n\\nCREATE TABLE microbiologyevents (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    charttime time,\\n    spec_type_desc text,\\n    org_name text\\n)\\n\\nCREATE TABLE d_icd_diagnoses (\\n    row_id number,\\n    icd9_code text,\\n    short_title text,\\n    long_title text\\n)\\n\\nCREATE TABLE inputevents_cv (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    icustay_id number,\\n    charttime time,\\n    itemid number,\\n    amount number\\n)\\n\\nCREATE TABLE d_labitems (\\n    row_id number,\\n    itemid number,\\n    label text\\n)\\n\\nCREATE TABLE outputevents (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    icustay_id number,\\n    charttime time,\\n    itemid number,\\n    value number\\n)\\n\\nCREATE TABLE chartevents (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    icustay_id number,\\n    itemid number,\\n    charttime time,\\n    valuenum number,\\n    valueuom text\\n)\\n\\nCREATE TABLE prescriptions (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    startdate time,\\n    enddate time,\\n    drug text,\\n    dose_val_rx text,\\n    dose_unit_rx text,\\n    route text\\n)\\n\\nCREATE TABLE admissions (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    admittime time,\\n    dischtime time,\\n    admission_type text,\\n    admission_location text,\\n    discharge_location text,\\n    insurance text,\\n    language text,\\n    marital_status text,\\n    ethnicity text,\\n    age number\\n)\\n\\nCREATE TABLE patients (\\n    row_id number,\\n    subject_id number,\\n    gender text,\\n    dob time,\\n    dod time\\n)\\n\\nCREATE TABLE d_icd_procedures (\\n    row_id number,\\n    icd9_code text,\\n    short_title text,\\n    long_title text\\n)\\n\\nCREATE TABLE cost (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    event_type text,\\n    event_id number,\\n    chargetime time,\\n    cost number\\n)\\n\\nCREATE TABLE icustays (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    icustay_id number,\\n    first_careunit text,\\n    last_careunit text,\\n    first_wardid number,\\n    last_wardid number,\\n    intime time,\\n    outtime time\\n)\\n\\nCREATE TABLE transfers (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    icustay_id number,\\n    eventtype text,\\n    careunit text,\\n    wardid number,\\n    intime time,\\n    outtime time\\n)\\n\\nCREATE TABLE labevents (\\n    row_id number,\\n    subject_id number,\\n    hadm_id number,\\n    itemid number,\\n    charttime time,\\n    valuenum number,\\n    valueuom text\\n)'],\n",
       " 'answer': ['SELECT \"2nd leg\" FROM table_56562 WHERE \"Team #1\" = \\'alemannia aachen\\'',\n",
       "  'SELECT \"Title\" FROM table_3706 WHERE \"No. in series\" = \\'9\\'',\n",
       "  'SELECT award FROM table_name_35 WHERE category = \"best actor\" AND year < 1988',\n",
       "  'SELECT crowd FROM table_name_62 WHERE away_team = \"north melbourne\"',\n",
       "  'SELECT date FROM table_name_58 WHERE runner_up_skip = \"stu harris\"',\n",
       "  'SELECT * FROM enrollments WHERE UserId = 102 AND DATEDIFF(YEAR, CreatedAt, GETDATE()) = 0;',\n",
       "  'SELECT time FROM table_name_75 WHERE notes = \"fa\" AND rank < 2',\n",
       "  \"SELECT COUNT(*) FROM labevents WHERE labevents.itemid IN (SELECT d_labitems.itemid FROM d_labitems WHERE d_labitems.label = 'mch') AND labevents.hadm_id IN (SELECT admissions.hadm_id FROM admissions WHERE admissions.subject_id = 15754) AND STRFTIME('%y', labevents.charttime) = '2104'\"]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['test'][0:8] # Kiá»ƒm tra má»™t máº«u trong táº­p test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb4957",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 5) HÃ€M TIá»€N Xá»¬ LÃ (TOKENIZE) + Táº O PROMPT\n",
    "# ==============================\n",
    "\n",
    "á» bÆ°á»›c nÃ y, cáº§n chuyá»ƒn Ä‘á»•i cÃ¡c bá»™ dá»¯ liá»‡u thÃ nh dáº¡ng hÆ°á»›ng dáº«n rÃµ rÃ ng cho mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM).\n",
    "\n",
    "Sau Ä‘Ã³, tiáº¿n hÃ nh tiá»n xá»­ lÃ½ dá»¯ liá»‡u prompt-response báº±ng cÃ¡ch mÃ£ hÃ³a (tokenize) Ä‘á»ƒ láº¥y ra cÃ¡c input_ids phá»¥c vá»¥ cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n.\n",
    "\n",
    "Ghi chÃº: Chuyá»ƒn dá»¯ liá»‡u dáº¡ng (context/question/answer) â†’ (input_ids/labels) cho T5\n",
    " Prompt dáº¡ng:\n",
    " Tables:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2bff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3165e441214221ae1f982a0e39b801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacf220103aa40bdb1fb4620d7f99a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160b9fc9b0ff4b9d84128a8482de6e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d4ebe2fda046f2bd6ed2c52b2f3af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/124561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48294fa991748baa496e6c358a877df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/17058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99650555e80346cdab90f5a17cea2979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenized dataset saved to cache.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'finetuned_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\n",
      "\u001b[0;32m/tmp/ipykernel_37/281804483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;32m     54\u001b[0m data_collator = DataCollatorForSeq2Seq(\n",
      "\n",
      "\u001b[1;32m     55\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinetuned_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"longest\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32m     58\u001b[0m )\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'finetuned_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "MAX_INPUT_LEN = 256   # Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a prompt (Tables + Question)\n",
    "MAX_LABEL_LEN = 128   # Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a Answer/SQL\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # Táº¡o prompt chuáº©n dáº¡ng:\n",
    "    # Tables: [schema]\n",
    "    # Question: [natural language question]\n",
    "    # Answer: [SQL query]\n",
    "    start_prompt = \"Tables:\\n\"\n",
    "    middle_prompt = \"\\n\\nQuestion:\\n\"\n",
    "    end_prompt = \"\\n\\nAnswer:\\n\"\n",
    "\n",
    "    # GhÃ©p tá»«ng cáº·p context + question thÃ nh má»™t prompt hoÃ n chá»‰nh\n",
    "    prompt = [\n",
    "        start_prompt + context + middle_prompt + question + end_prompt\n",
    "        for context, question in zip(example[\"context\"], example[\"question\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra vá»›i padding Ä‘á»™ng + cáº¯t Ä‘á»™ dÃ i há»£p lÃ½\n",
    "    model_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=False,  # ğŸ”» khÃ´ng Ã©p vá» max_length\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        example[\"answer\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LABEL_LEN,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ”¹ Táº O DATASET TOKENIZED (CÃ“ CACHE)\n",
    "# =========================================================\n",
    "try:\n",
    "    tokenized_datasets = load_from_disk(\"tokenized_datasets\")\n",
    "    print(\"âœ… Loaded tokenized dataset from cache.\")\n",
    "except:\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"question\", \"context\", \"answer\"]\n",
    "    )\n",
    "    tokenized_datasets.save_to_disk(\"tokenized_datasets\")\n",
    "    print(\"âœ… Tokenized dataset saved to cache.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9adc835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiá»ƒn thá»‹ thÃ´ng tin tokenized_datasets kiá»ƒm tra nhanh khÃ³a/cáº¥u trÃºc trÆ°á»›c khi train\n",
    "print(tokenized_datasets.keys()) # Hiá»ƒn thá»‹ cÃ¡c split cÃ³ trong tokenized_datasets\n",
    "print(tokenized_datasets['train'][0].keys()) # Hiá»ƒn thá»‹ cÃ¡c khÃ³a trong má»™t máº«u cá»§a táº­p train\n",
    "print(tokenized_datasets['train'][0]['input_ids'][:10]) # Hiá»ƒn thá»‹ 10 token Ä‘áº§u tiÃªn cá»§a input_ids\n",
    "print(tokenized_datasets['train'][0]['labels'][:10]) # Hiá»ƒn thá»‹ 10 token Ä‘áº§u tiÃªn cá»§a labels\n",
    "print(tokenized_datasets) # Hiá»ƒn thá»‹ thÃ´ng tin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1dd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\") # In kÃ­ch thÆ°á»›c táº­p train (sá»‘ máº«u, sá»‘ cá»™t)\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\") # In kÃ­ch thÆ°á»›c táº­p validation (sá»‘ máº«u, sá»‘ cá»™t)\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\") # In kÃ­ch thÆ°á»›c táº­p test (sá»‘ máº«u, sá»‘ cá»™t)\n",
    "\n",
    "print(tokenized_datasets) # Hiá»ƒn thá»‹ thÃ´ng tin tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29472f8",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 7) KIá»‚M THá»¬ ZERO-SHOT Vá»šI MODEL Gá»C (BASELINE)\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f846e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # ThÆ° viá»‡n PyTorch tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½ GPU, tensor\n",
    "\n",
    "# Äá»‹nh nghÄ©a prompt & answer máº«u Ä‘á»ƒ test nhanh (ngÆ°á»i dÃ¹ng cáº§n thay báº±ng vÃ­ dá»¥ tháº­t)\n",
    "prompt = \"Your input prompt here\"  # VÃ­ dá»¥: Tables + Question (cáº§n thay báº±ng prompt thá»±c)\n",
    "answer = \"Expected human response here\"  # VÃ­ dá»¥: cÃ¢u SQL chuáº©n tÆ°Æ¡ng á»©ng (Ä‘Ã¡p Ã¡n chuáº©n)\n",
    "\n",
    "# Äáº£m báº£o model vÃ  input cÃ¹ng trÃªn má»™t thiáº¿t bá»‹ (CPU hoáº·c GPU) náº¿u cÃ³\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Di chuyá»ƒn model sang thiáº¿t bá»‹ phÃ¹ há»£p (device = CPU hoáº·c GPU)\n",
    "original_model.to(device)\n",
    "\n",
    "# Tokenize Ä‘áº§u vÃ o input vÃ  chuyá»ƒn nÃ³ sang cÃ¹ng thiáº¿t bá»‹ vá»›i model\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Sinh Ä‘áº§u ra zero-shot tá»« model gá»‘c (chÆ°a fine-tune)\n",
    "output = tokenizer.decode( # dÃ¹ng tokenizer.decode Ä‘á»ƒ giáº£i mÃ£ token thÃ nh text\n",
    "    original_model.generate( # dÃ¹ng phÆ°Æ¡ng thá»©c generate Ä‘á»ƒ sinh text\n",
    "        inputs[\"input_ids\"], # input_ids cá»§a prompt Ä‘Ã£ tokenized\n",
    "        max_new_tokens=200, # Giá»›i háº¡n tá»‘i Ä‘a 200 token má»›i sinh\n",
    "    )[0], # Láº¥y máº£ng token Ä‘áº§u tiÃªn trong batch (á»Ÿ Ä‘Ã¢y batch size=1)\n",
    "    skip_special_tokens=True # Bá» qua cÃ¡c token Ä‘áº·c biá»‡t khi giáº£i mÃ£\n",
    ")\n",
    "\n",
    "# In káº¿t quáº£ zero-shot baseline\n",
    "dash_line = '-' * 100 \n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}') \n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN ANSWER:\\n{answer}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85784b2d",
   "metadata": {},
   "source": [
    "# ==============================\n",
    "# 8) HUáº¤N LUYá»†N FULL FINE-TUNE (KAGGLE GPU)\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a227a69",
   "metadata": {},
   "source": [
    "\n",
    "**Giáº£i thÃ­ch:**\n",
    "\n",
    "- **5e-3**: ÄÃ¢y lÃ  giÃ¡ trá»‹ learning rate (tá»‘c Ä‘á»™ há»c) Ä‘Æ°á»£c sá»­ dá»¥ng khi huáº¥n luyá»‡n mÃ´ hÃ¬nh. GiÃ¡ trá»‹ nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n tá»‘c Ä‘á»™ cáº­p nháº­t trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh há»c.\n",
    "- **Thá»i gian huáº¥n luyá»‡n**: Tá»•ng thá»i gian Ä‘á»ƒ hoÃ n thÃ nh quÃ¡ trÃ¬nh fine-tune mÃ´ hÃ¬nh trÃªn GPU cá»§a Kaggle. Náº¿u dÃ¹ng mÃ¡y cÃ¡ nhÃ¢n (PC) khÃ´ng cÃ³ Ä‘á»§ bá»™ nhá»› CUDA thÃ¬ sáº½ khÃ´ng thá»ƒ huáº¥n luyá»‡n vá»›i táº­p dá»¯ liá»‡u lá»›n.\n",
    "- **Training Loss**: Äá»™ lá»—i (loss) trÃªn táº­p huáº¥n luyá»‡n. GiÃ¡ trá»‹ cÃ ng nhá» chá»©ng tá» mÃ´ hÃ¬nh há»c tá»‘t trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n.\n",
    "- **Validation Loss**: Äá»™ lá»—i trÃªn táº­p kiá»ƒm thá»­ (validation). GiÃ¡ trá»‹ nÃ y dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u chÆ°a tá»«ng tháº¥y. Náº¿u validation loss tháº¥p vÃ  gáº§n vá»›i training loss, mÃ´ hÃ¬nh khÃ´ng bá»‹ overfit.\n",
    "\n",
    "**Káº¿t luáº­n:**  \n",
    "MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i tá»‘c Ä‘á»™ há»c 5e-3, thá»i gian gáº§n 3 tiáº¿ng trÃªn GPU cá»§a Kaggle. Káº¿t quáº£ training loss vÃ  validation loss Ä‘á»u tháº¥p, chá»©ng tá» mÃ´ hÃ¬nh há»c tá»‘t vÃ  cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t trÃªn dá»¯ liá»‡u kiá»ƒm thá»­."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i thÃªm Ä‘á»ƒ cháº¡y service náº¿u cáº§n (khÃ´ng báº¯t buá»™c)\n",
    "!pip install fastapi uvicorn transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27febc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil # ThÆ° viá»‡n shutil Ä‘á»ƒ nÃ©n file zip\n",
    "\n",
    "# NÃ©n thÆ° má»¥c model Ä‘Ã£ fine-tune Ä‘á»ƒ tiá»‡n download tá»« Kaggle\n",
    "shutil.make_archive(\n",
    "    '/kaggle/working/sql_t5_finetuned',  # tÃªn file zip Ä‘áº§u ra\n",
    "    'zip',                               # Ä‘á»‹nh dáº¡ng nÃ©n\n",
    "    '/kaggle/working/finetuned_model_2_epoch'  # thÆ° má»¥c model tháº­t sá»±\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fabeb0",
   "metadata": {},
   "source": [
    "**NÃ©n toÃ n bá»™ dá»¯ liá»‡u quan trá»ng gá»“m model, dataset gá»‘c, tokenized dataset, checkpoint training â€” Ä‘á»ƒ táº£i 1 láº§n duy nháº¥t.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b970d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# ÄÆ°á»ng dáº«n thÆ° má»¥c lÃ m viá»‡c\n",
    "base_dir = \"/kaggle/working\"\n",
    "\n",
    "# ÄÆ°á»ng dáº«n file zip Ä‘áº§u ra\n",
    "output_zip = f\"{base_dir}/olms_sql_full_backup\"\n",
    "\n",
    "# NÃ©n toÃ n bá»™ working directory (model + dataset + checkpoint)\n",
    "shutil.make_archive(output_zip, 'zip', base_dir)\n",
    "\n",
    "print(\"ÄÃ£ nÃ©n toÃ n bá»™ dá»¯ liá»‡u thÃ nh cÃ´ng!\")\n",
    "print(f\" File nÃ©n náº±m táº¡i: {output_zip}.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80308aee",
   "metadata": {},
   "source": [
    "# ğŸ” Resume Training (Strict) â€” Use Kaggle **folder** dataset (no zip)\n",
    "> **LÆ°u Ã½:** Pháº§n dÆ°á»›i **chá»‰ thÃªm cell má»›i Ä‘á»ƒ resume**, **khÃ´ng** thay Ä‘á»•i báº¥t ká»³ tham sá»‘ huáº¥n luyá»‡n cÅ© nÃ o.\n",
    "> `TrainingArguments` **Ä‘Æ°á»£c giá»¯ nguyÃªn** (dÃ¹ng láº¡i biáº¿n `args` Ä‘Ã£ khai bÃ¡o á»Ÿ trÃªn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f1474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint copied to: /kaggle/working/sql_t5_finetuned\n",
      "HAS_TOKENIZED = False | HAS_RAW(merged_dataset) = True\n"
     ]
    }
   ],
   "source": [
    "# === [CHANGE] Chá»‰ Ä‘á»‹nh dataset chá»©a checkpoint (model v3) vÃ  merged_dataset má»›i (á»Ÿ WORKING) ===\n",
    "DATASET_MODEL = \"/kaggle/input/sql-t5-finetuned-v3\"   # âœ… model v3: files náº±m á»Ÿ ROOT\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "# === [CHANGE] TÃ¬m checkpoint trong dataset model v3 (files á»Ÿ root)\n",
    "if os.path.exists(os.path.join(DATASET_MODEL, \"config.json\")) and \\\n",
    "   (os.path.exists(os.path.join(DATASET_MODEL, \"pytorch_model.bin\")) or \n",
    "    os.path.exists(os.path.join(DATASET_MODEL, \"model.safetensors\"))):\n",
    "    CKPT_SRC = DATASET_MODEL                     # âœ… láº¥y trá»±c tiáº¿p root lÃ m checkpoint src\n",
    "else:\n",
    "    raise FileNotFoundError(\"KhÃ´ng tháº¥y checkpoint (config.json + weights) trong sql-t5-finetuned-v3.\")\n",
    "\n",
    "# === Copy checkpoint sang vÃ¹ng ghi Ä‘Æ°á»£c Ä‘á»ƒ resume (KHÃ”NG Ä‘á»•i tÃªn biáº¿n Ä‘Ã­ch) ===\n",
    "WORK      = \"/kaggle/working\"\n",
    "CKPT_WORK = os.path.join(WORK, \"sql_t5_finetuned\")\n",
    "os.makedirs(CKPT_WORK, exist_ok=True)\n",
    "shutil.copytree(CKPT_SRC, CKPT_WORK, dirs_exist_ok=True)\n",
    "print(\"Checkpoint copied to:\", CKPT_WORK)\n",
    "\n",
    "# === Æ¯u tiÃªn dÃ¹ng merged_dataset vá»«a merge á»Ÿ WORKING ===\n",
    "# Náº¿u chÆ°a cÃ³, nhÆ°ng cÃ³ file jsonl má»›i (olsm-clean.jsonl) thÃ¬ tá»± build merged_dataset tá»‘i thiá»ƒu rá»“i dÃ¹ng.\n",
    "TOKENIZED_INPUT = os.path.join(DATASET_MODEL, \"tokenized_datasets\")  # bá»™ tokenized cÅ© (KHÃ”NG dÃ¹ng)\n",
    "RAW_DIR         = \"merged_dataset\"                                    # mong muá»‘n: save_to_disk á»Ÿ WORKING\n",
    "JSONL_FALLBACK  = \"/kaggle/working/olsm-clean.jsonl\"                  # náº¿u chÆ°a cÃ³ merged_dataset\n",
    "\n",
    "USE_OLD_TOKENIZED = False\n",
    "HAS_TOKENIZED   = os.path.exists(TOKENIZED_INPUT) and USE_OLD_TOKENIZED\n",
    "HAS_RAW         = os.path.exists(RAW_DIR)\n",
    "\n",
    "print(\"HAS_TOKENIZED =\", HAS_TOKENIZED, \"| HAS_RAW(merged_dataset) =\", HAS_RAW)\n",
    "\n",
    "# === [CHANGE NHáº¸] Fallback: náº¿u chÆ°a cÃ³ merged_dataset nhÆ°ng cÃ³ jsonl, convert nhanh thÃ nh merged_dataset ===\n",
    "if (not HAS_RAW) and os.path.exists(JSONL_FALLBACK):\n",
    "    from datasets import load_dataset\n",
    "    print(\"âš ï¸  merged_dataset chÆ°a cÃ³. Äang build táº¡m tá»«:\", JSONL_FALLBACK)\n",
    "    ds_tmp = load_dataset(\"json\", data_files=JSONL_FALLBACK, split=\"train\")\n",
    "    # Ä‘áº£m báº£o 3 cá»™t chuáº©n\n",
    "    need_cols = {\"question\",\"context\",\"answer\"}\n",
    "    assert need_cols.issubset(ds_tmp.column_names), f\"JSONL thiáº¿u cá»™t. CÃ³: {ds_tmp.column_names}\"\n",
    "    # chia 80/10/10 vÃ  save_to_disk Ä‘á»ƒ cÃ¡c cell sau load_from_disk nhÆ° cÅ©\n",
    "    _spl = ds_tmp.train_test_split(test_size=0.2, seed=42)\n",
    "    _train = _spl[\"train\"]\n",
    "    _tmp2  = _spl[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    _test, _val = _tmp2[\"train\"], _tmp2[\"test\"]\n",
    "    from datasets import DatasetDict\n",
    "    ds_built = DatasetDict({\"train\": _train, \"test\": _test, \"validation\": _val})\n",
    "    ds_built.save_to_disk(RAW_DIR)\n",
    "    HAS_RAW = True\n",
    "    print(\"âœ… ÄÃ£ build vÃ  lÆ°u merged_dataset/ (tá»‘i thiá»ƒu) tá»« jsonl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98561a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model/tokenizer from: /kaggle/working/sql_t5_finetuned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === [NO CHANGE TO LOGIC] Load tokenizer & model tá»« checkpoint cÅ© ===\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(CKPT_WORK)\n",
    "model     = T5ForConditionalGeneration.from_pretrained(CKPT_WORK)\n",
    "model.config.use_cache = False  # giá»¯ nguyÃªn an toÃ n cho Trainer\n",
    "print(\"Loaded model/tokenizer from:\", CKPT_WORK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1d9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e091ea34bbb43d0bc261dc632de64a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c280101e4c4498d95afcdb7c5f9647c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized from merged_dataset: merged_dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6945c7fb0e1c4329bf2cdcd44173bb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/124561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098d151005ba4167b797d0169899478f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new tokenized dataset to: /kaggle/working/tokenized_datasets_v4\n",
      "Sizes: 124561 15001\n"
     ]
    }
   ],
   "source": [
    "# === Chuáº©n bá»‹ dá»¯ liá»‡u: dÃ¹ng merged_dataset má»›i á»Ÿ WORKING ===\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "if HAS_TOKENIZED:\n",
    "    # (KhÃ´ng dÃ¹ng vÃ¬ USE_OLD_TOKENIZED=False)\n",
    "    ds = load_from_disk(TOKENIZED_INPUT)\n",
    "    train_ds = ds[\"train\"]\n",
    "    eval_ds  = ds.get(\"validation\", None) or ds.get(\"eval\", None)\n",
    "    print(\"Loaded tokenized_datasets from:\", TOKENIZED_INPUT)\n",
    "\n",
    "elif HAS_RAW:\n",
    "    # merged_dataset lÃ  DatasetDict Ä‘Ã£ save_to_disk\n",
    "    ds = load_from_disk(RAW_DIR)\n",
    "    raw_train = ds[\"train\"]\n",
    "    raw_val   = ds[\"validation\"]\n",
    "\n",
    "    # TÃªn cá»™t chuáº©n\n",
    "    INPUT_Q  = \"question\"\n",
    "    INPUT_C  = \"context\"\n",
    "    TARGET_A = \"answer\"\n",
    "\n",
    "    # [CHANGE] DÃ¹ng Ä‘Ãºng prompt Ä‘ang infer: \"Tables:\\n{context}\\n\\nQuestion:\\n{q}\\n\\nAnswer:\\n\"\n",
    "    def to_prompt(q, ctx):\n",
    "        ctx = (ctx or \"\").strip()\n",
    "        if ctx:\n",
    "            return f\"Tables:\\n{ctx}\\n\\nQuestion:\\n{q}\\n\\nAnswer:\\n\"\n",
    "        # fallback náº¿u context trá»‘ng\n",
    "        return f\"translate to sql: {q}\\nAnswer:\\n\"\n",
    "\n",
    "    # [CHANGE] Bá» as_target_tokenizer (API cÅ©). DÃ¹ng text_target cho nhÃ£n.\n",
    "    def preprocess(examples):\n",
    "        inputs  = [to_prompt(q, c) for q, c in zip(examples[INPUT_Q], examples[INPUT_C])]\n",
    "        targets = examples[TARGET_A]\n",
    "        model_inputs = tokenizer(inputs, max_length=256, truncation=True)  # padding do collator xá»­ lÃ½\n",
    "        labels = tokenizer(text_target=targets, max_length=256, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    train_ds = raw_train.map(preprocess, batched=True, remove_columns=raw_train.column_names)\n",
    "    eval_ds  = raw_val.map(preprocess,   batched=True, remove_columns=raw_val.column_names)\n",
    "    print(\"Tokenized from merged_dataset:\", RAW_DIR)\n",
    "\n",
    "    # lÆ°u Ä‘á»ƒ láº§n sau load nhanh\n",
    "    from datasets import DatasetDict\n",
    "    tok_out = \"/kaggle/working/tokenized_datasets_v4\"\n",
    "    DatasetDict({\"train\": train_ds, \"validation\": eval_ds}).save_to_disk(tok_out)\n",
    "    print(\"Saved new tokenized dataset to:\", tok_out)\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"KhÃ´ng tÃ¬m tháº¥y merged_dataset má»›i á»Ÿ WORKING.\")\n",
    "\n",
    "print(\"Sizes:\", len(train_ds), len(eval_ds) if eval_ds is not None else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1651d5d",
   "metadata": {},
   "source": [
    "Tá»± táº¡o TrainingArguments má»›i (nhÆ°ng pháº£i dÃ¹ng Ä‘Ãºng biáº¿n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014267ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['added_tokens.json', 'special_tokens_map.json', 'config.json', 'generation_config.json', 'model.safetensors', 'spiece.model', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(CKPT_WORK)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55f922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¾ Using dataset source: train_ds (tokenized_datasets)\n",
      "ğŸ”¢ Train size: 124561 | Eval size: 15001\n",
      "ğŸ“ˆ Steps/epoch (theory): 7786 with effective batch = 16\n",
      "ğŸ“¦ CKPT_WORK: /kaggle/working/sql_t5_finetuned\n",
      "CKPT files (head): ['added_tokens.json', 'special_tokens_map.json', 'config.json', 'generation_config.json', 'model.safetensors', 'spiece.model', 'tokenizer_config.json']\n",
      "âš™ï¸ gradient_checkpointing: False\n",
      "ğŸ‘€ Sample[0] keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "import math, os, json, time, torch\n",
    "\n",
    "# 1) Kiá»ƒm tra Ä‘Æ°á»ng dáº«n & loáº¡i dá»¯ liá»‡u Ä‘ang dÃ¹ng\n",
    "using_tokenized = 'train_ds' in globals() and hasattr(train_ds, '__len__')\n",
    "src = \"train_ds (tokenized_datasets)\" if using_tokenized else \"tokenized_datasets['train']\" if 'tokenized_datasets' in globals() else \"UNKNOWN\"\n",
    "\n",
    "if using_tokenized:\n",
    "    N_train = len(train_ds)\n",
    "    N_eval  = len(eval_ds) if 'eval_ds' in globals() and eval_ds is not None else 0\n",
    "else:\n",
    "    assert 'tokenized_datasets' in globals(), \"KhÃ´ng tÃ¬m tháº¥y biáº¿n train_ds hoáº·c tokenized_datasets.\"\n",
    "    N_train = len(tokenized_datasets[\"train\"])\n",
    "    N_eval  = len(tokenized_datasets[\"validation\"]) if \"validation\" in tokenized_datasets else 0\n",
    "\n",
    "print(f\"ğŸ§¾ Using dataset source: {src}\")\n",
    "print(f\"ğŸ”¢ Train size: {N_train} | Eval size: {N_eval}\")\n",
    "\n",
    "# 2) TÃ­nh steps/epoch theo args hiá»‡n táº¡i\n",
    "bs = 4   # per_device_train_batch_size (báº¡n Ä‘ang Ä‘á»ƒ 4)\n",
    "ga = 4   # gradient_accumulation_steps (báº¡n Ä‘ang Ä‘á»ƒ 4)\n",
    "world = 1  # Kaggle T4 = 1 GPU\n",
    "steps_per_epoch = math.ceil( math.ceil(N_train / (bs*world)) / ga )\n",
    "print(f\"ğŸ“ˆ Steps/epoch (theory): {steps_per_epoch} with effective batch = {bs*ga*world}\")\n",
    "\n",
    "# 3) Kiá»ƒm tra model & checkpoint hiá»‡n táº¡i\n",
    "print(\"ğŸ“¦ CKPT_WORK:\", CKPT_WORK if 'CKPT_WORK' in globals() else \"(not set)\")\n",
    "if 'CKPT_WORK' in globals() and os.path.exists(CKPT_WORK):\n",
    "    print(\"CKPT files (head):\", os.listdir(CKPT_WORK)[:12])\n",
    "\n",
    "# 4) Check gradient checkpointing on/off\n",
    "if hasattr(model, \"is_gradient_checkpointing\"):\n",
    "    print(\"âš™ï¸ gradient_checkpointing:\", model.is_gradient_checkpointing)\n",
    "else:\n",
    "    print(\"âš™ï¸ gradient_checkpointing: unknown flag; assuming enabled if you called gradient_checkpointing_enable()\")\n",
    "\n",
    "# 5) In thá»­ 2 máº«u Ä‘áº§u Ä‘á»ƒ cháº¯c cháº¯n Ä‘Ãºng táº­p\n",
    "if using_tokenized:\n",
    "    try:\n",
    "        print(\"ğŸ‘€ Sample[0] keys:\", train_ds[0].keys())\n",
    "    except Exception as e:\n",
    "        print(\"Sample preview error:\", e)\n",
    "else:\n",
    "    try:\n",
    "        print(\"ğŸ‘€ Sample[0] keys:\", tokenized_datasets[\"train\"][0].keys())\n",
    "    except Exception as e:\n",
    "        print(\"Sample preview error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eba7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cháº¡y cell nÃ y ngay trÆ°á»›c khi táº¡o Trainer (KHÃ”NG Ä‘á»•i args)\n",
    "model.gradient_checkpointing_disable()   # <- táº¯t Ä‘á»ƒ tÄƒng tá»‘c\n",
    "model.config.use_cache = False           # giá»¯ nhÆ° cÅ© Ä‘á»ƒ khÃ´ng bÆ¡m KV-cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876828c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "\n",
    "# Báº­t TF32 trÃªn GPU (PyTorch 2.x, T4/AMPERE há»— trá»£) â€“ tÄƒng tá»‘c matmul\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")  # náº¿u PyTorch há»— trá»£\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Cho phÃ©p tokenizer xá»­ lÃ½ song song (giáº£m overhead CPU)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Náº¿u kernel bá»‹ ngháº½n CPU, giá»›i háº¡n threads Ä‘á»ƒ á»•n Ä‘á»‹nh (tuá»³ mÃ¡y)\n",
    "try:\n",
    "    torch.set_num_threads(2)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Náº¿u VRAM Ä‘á»§: táº¯t gradient checkpointing Ä‘á»ƒ tÄƒng tá»‘c\n",
    "try:\n",
    "    model.gradient_checkpointing_disable()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a32d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google-t5/t5-small\"  # fallback base\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(CKPT_WORK).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(CKPT_WORK)\n",
    "to_train = True  # muá»‘n tiáº¿p tá»¥c train thÃªm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ac8ae",
   "metadata": {},
   "source": [
    "# **KIá»‚M TRA TRÆ¯á»šC KHI TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430461aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ====== PRE-TRAIN SANITY CHECK ======\n",
      "ğŸ§  Device: CUDA | GPU: Tesla T4\n",
      "ğŸ“¦ Checkpoint in use: /kaggle/working/sql_t5_finetuned\n",
      "ğŸ“ Sample files: ['added_tokens.json', 'special_tokens_map.json', 'config.json', 'generation_config.json', 'model.safetensors', 'spiece.model', 'tokenizer_config.json']\n",
      "âœ… Model loaded: T5ForConditionalGeneration\n",
      "âœ… Tokenizer vocab size: 32100\n",
      "ğŸ“Š Dataset source: 'train_ds' (custom tokenized)\n",
      "   â†’ Train samples: 124,561\n",
      "   â†’ Eval samples:  15,001\n",
      "ğŸ‘€ Example keys: ['input_ids', 'attention_mask', 'labels']\n",
      "âœ… Tokenized format confirmed (input_ids + labels).\n",
      "ğŸ“ˆ Estimated steps/epoch: 7,786 (effective batch = 16)\n",
      "ğŸ” Sanity check OK âœ… â€” Ready to train if everything looks correct.\n"
     ]
    }
   ],
   "source": [
    "import os, torch, math\n",
    "\n",
    "print(\"ğŸ” ====== PRE-TRAIN SANITY CHECK ======\")\n",
    "\n",
    "# 1ï¸âƒ£ Kiá»ƒm tra thiáº¿t bá»‹ vÃ  model checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ§  Device: {device.upper()} | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only'}\")\n",
    "\n",
    "if 'CKPT_WORK' in globals() and os.path.exists(CKPT_WORK):\n",
    "    print(f\"ğŸ“¦ Checkpoint in use: {CKPT_WORK}\")\n",
    "    ckpt_files = os.listdir(CKPT_WORK)[:10]\n",
    "    print(\"ğŸ“ Sample files:\", ckpt_files)\n",
    "else:\n",
    "    print(\"âš ï¸ CKPT_WORK not found â€” model may be base or loaded directly.\")\n",
    "\n",
    "# 2ï¸âƒ£ Kiá»ƒm tra model & tokenizer\n",
    "try:\n",
    "    _ = finetuned_model.state_dict()\n",
    "    print(\"âœ… Model loaded:\", finetuned_model.__class__.__name__)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"âŒ Model not loaded properly: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"âœ… Tokenizer vocab size:\", len(tokenizer))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"âŒ Tokenizer not found: {e}\")\n",
    "\n",
    "# 3ï¸âƒ£ Kiá»ƒm tra dataset\n",
    "if 'train_ds' in globals():\n",
    "    ds_train = train_ds\n",
    "    ds_eval  = eval_ds if 'eval_ds' in globals() else None\n",
    "    print(\"ğŸ“Š Dataset source: 'train_ds' (custom tokenized)\")\n",
    "elif 'tokenized_datasets' in globals():\n",
    "    ds_train = tokenized_datasets[\"train\"]\n",
    "    ds_eval  = tokenized_datasets.get(\"validation\", None)\n",
    "    print(\"ğŸ“Š Dataset source: 'tokenized_datasets'\")\n",
    "else:\n",
    "    raise RuntimeError(\"âŒ No dataset found! You must load tokenized dataset first.\")\n",
    "\n",
    "print(f\"   â†’ Train samples: {len(ds_train):,}\")\n",
    "if ds_eval is not None:\n",
    "    print(f\"   â†’ Eval samples:  {len(ds_eval):,}\")\n",
    "else:\n",
    "    print(\"   â†’ Eval samples:  (None â€” skipped eval)\")\n",
    "\n",
    "# 4ï¸âƒ£ Kiá»ƒm tra 1 sample ngáº«u nhiÃªn\n",
    "try:\n",
    "    ex = ds_train[0]\n",
    "    print(\"ğŸ‘€ Example keys:\", list(ex.keys()))\n",
    "    if \"input_ids\" in ex and \"labels\" in ex:\n",
    "        print(\"âœ… Tokenized format confirmed (input_ids + labels).\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Non-tokenized format â€” báº¡n cÃ³ thá»ƒ cáº§n map(preprocess) láº¡i.\")\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Could not preview sample:\", e)\n",
    "\n",
    "# 5ï¸âƒ£ TÃ­nh toÃ¡n sá»‘ bÆ°á»›c má»—i epoch\n",
    "bs = 4\n",
    "ga = 4\n",
    "world = 1\n",
    "steps_per_epoch = math.ceil(len(ds_train) / (bs * ga * world))\n",
    "print(f\"ğŸ“ˆ Estimated steps/epoch: {steps_per_epoch:,} (effective batch = {bs*ga})\")\n",
    "\n",
    "print(\"ğŸ” Sanity check OK âœ… â€” Ready to train if everything looks correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb5a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Trainer runtime args check ====\n",
      "per_device_train_batch_size: 4\n",
      "gradient_accumulation_steps: 1\n",
      "world_size: 1\n",
      "len(train_ds): 124561\n",
      "len(train_dataloader) (num mini-batches): 15571\n",
      "actual batch size from tensor: 8\n",
      "=> expected optimizer steps (ceil): 15571\n"
     ]
    }
   ],
   "source": [
    "import math, torch\n",
    "\n",
    "print(\"==== Trainer runtime args check ====\")\n",
    "print(\"per_device_train_batch_size:\", trainer.args.per_device_train_batch_size)\n",
    "print(\"gradient_accumulation_steps:\", trainer.args.gradient_accumulation_steps)\n",
    "print(\"world_size:\", trainer.args.world_size)\n",
    "\n",
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "print(\"len(train_ds):\", len(train_data_for_trainer))\n",
    "print(\"len(train_dataloader) (num mini-batches):\", len(dl))\n",
    "print(\"actual batch size from tensor:\", batch[\"input_ids\"].shape[0])\n",
    "print(\"=> expected optimizer steps (ceil):\",\n",
    "      math.ceil(len(dl)/trainer.args.gradient_accumulation_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RESET TRAINER STATE (quan trá»ng) ======\n",
    "import torch, gc\n",
    "for v in [\"trainer\", \"args\", \"training_args\"]:\n",
    "    if v in globals():\n",
    "        del globals()[v]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9baa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== SAFETY: Ä‘áº£m báº£o dataset Ä‘Ã£ tokenized ======\n",
    "assert 'train_ds' in globals() or 'tokenized_datasets' in globals(), \"âŒ ChÆ°a cÃ³ tokenized dataset.\"\n",
    "\n",
    "train_data_for_trainer = train_ds if 'train_ds' in globals() else tokenized_datasets[\"train\"]\n",
    "eval_data_for_trainer  = (eval_ds if 'eval_ds' in globals()\n",
    "                          else tokenized_datasets.get(\"validation\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Cáº¤U HÃŒNH Há»ŒC SÃ‚U (deep) ======\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "output_dir = f\"./sql-training-deep\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=3e-3,                 # deep hÆ¡n -> háº¡ LR nháº¹ cho á»•n Ä‘á»‹nh\n",
    "    num_train_epochs=3,                 # há»c ká»¹ hÆ¡n\n",
    "    per_device_train_batch_size=4,      # batch nhá» giÃºp há»c tinh hÆ¡n\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,      # gá»™p Ã­t hÆ¡n -> nhiá»u optimizer steps hÆ¡n\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",           # giá»¯ nháº¹\n",
    "    save_strategy=\"steps\",              #  lÆ°u theo bÆ°á»›c\n",
    "    save_steps=1000,                    #  má»—i 1000 bÆ°á»›c lÆ°u 1 ckpt\n",
    "    fp16=True,                          # tiáº¿t kiá»‡m VRAM\n",
    "    optim=\"adafactor\",                  # tiáº¿t kiá»‡m bá»™ nhá»›\n",
    "    report_to=\"none\",\n",
    "    group_by_length=False,              # TRÃNH sampler tá»± gom Ä‘á»™ dÃ i lÃ m báº¡n khÃ³ Ä‘á»c batch/steps\n",
    "    dataloader_drop_last=False,\n",
    "    dataloader_num_workers=0,           # 0 náº¿u lá»—i mÃ´i trÆ°á»ng\n",
    "    dataloader_pin_memory=False,\n",
    "    seed=42,\n",
    "    save_total_limit=2,          # chá»‰ giá»¯ láº¡i 2 checkpoint gáº§n nháº¥t\n",
    "    warmup_ratio=0.03,           # ~3% bÆ°á»›c Ä‘áº§u giáº£m rá»§i ro â€œgiáº­tâ€ LR\n",
    "    lr_scheduler_type=\"linear\",  # lá»‹ch trÃ¬nh LR Ä‘Æ¡n giáº£n, á»•n Ä‘á»‹nh\n",
    ")\n",
    "\n",
    "# Data collator cho seq2seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=finetuned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94940c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== KHá»I Táº O TRAINER ======\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data_for_trainer,\n",
    "    # eval_dataset khÃ´ng truyá»n vÃ¬ evaluation_strategy=\"no\"\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Runtime sanity check ====\n",
      "Train size: 124561\n",
      "per_device_train_batch_size: 4\n",
      "gradient_accumulation_steps: 1\n",
      "len(train_dataloader): 15571\n",
      "actual batch size (tensor): 8\n",
      "=> expected optimizer steps/epoch: 15571\n"
     ]
    }
   ],
   "source": [
    "# ====== SANITY CHECK TRÆ¯á»šC KHI TRAIN ======\n",
    "import math\n",
    "dl = trainer.get_train_dataloader()\n",
    "first = next(iter(dl))\n",
    "print(\"==== Runtime sanity check ====\")\n",
    "print(\"Train size:\", len(train_data_for_trainer))\n",
    "print(\"per_device_train_batch_size:\", trainer.args.per_device_train_batch_size)\n",
    "print(\"gradient_accumulation_steps:\", trainer.args.gradient_accumulation_steps)\n",
    "print(\"len(train_dataloader):\", len(dl))\n",
    "print(\"actual batch size (tensor):\", first[\"input_ids\"].shape[0])\n",
    "steps_per_epoch = math.ceil(len(dl) / trainer.args.gradient_accumulation_steps)\n",
    "print(\"=> expected optimizer steps/epoch:\", steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39087dfe",
   "metadata": {},
   "source": [
    "# TRAIN 3EP - 2.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95866806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting fresh.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46713' max='46713' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46713/46713 2:21:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.176800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.221400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.243400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.276500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>0.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>0.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>0.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>0.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>0.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>0.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>0.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>0.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>0.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>0.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>0.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>0.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>0.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>0.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>0.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.257600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>0.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>0.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>0.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>0.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.266300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>0.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>0.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>0.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>0.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>0.238500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>0.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>0.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>0.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>0.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>0.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>0.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>0.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>0.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>0.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>0.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>0.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>0.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>0.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>0.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>0.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>0.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>0.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>0.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>0.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>0.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>0.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>0.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>0.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20350</td>\n",
       "      <td>0.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20450</td>\n",
       "      <td>0.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20550</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20650</td>\n",
       "      <td>0.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20750</td>\n",
       "      <td>0.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20850</td>\n",
       "      <td>0.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20950</td>\n",
       "      <td>0.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21050</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21150</td>\n",
       "      <td>0.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>0.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21350</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21450</td>\n",
       "      <td>0.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21550</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21650</td>\n",
       "      <td>0.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21750</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21850</td>\n",
       "      <td>0.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21950</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22050</td>\n",
       "      <td>0.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22150</td>\n",
       "      <td>0.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22250</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22350</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22450</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22550</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22650</td>\n",
       "      <td>0.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22750</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22850</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.181600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22950</td>\n",
       "      <td>0.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23050</td>\n",
       "      <td>0.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23150</td>\n",
       "      <td>0.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23250</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23350</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23450</td>\n",
       "      <td>0.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23550</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23650</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23750</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23850</td>\n",
       "      <td>0.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23950</td>\n",
       "      <td>0.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24050</td>\n",
       "      <td>0.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24150</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24250</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24350</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24450</td>\n",
       "      <td>0.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24550</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24650</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24750</td>\n",
       "      <td>0.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24850</td>\n",
       "      <td>0.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24950</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25050</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25150</td>\n",
       "      <td>0.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25250</td>\n",
       "      <td>0.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>0.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25350</td>\n",
       "      <td>0.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25450</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25550</td>\n",
       "      <td>0.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25650</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25750</td>\n",
       "      <td>0.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25850</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25950</td>\n",
       "      <td>0.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26050</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>0.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26150</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26250</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26350</td>\n",
       "      <td>0.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26450</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26550</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26650</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>0.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26750</td>\n",
       "      <td>0.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26850</td>\n",
       "      <td>0.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26950</td>\n",
       "      <td>0.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27050</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>0.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27150</td>\n",
       "      <td>0.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27250</td>\n",
       "      <td>0.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>0.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27350</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27450</td>\n",
       "      <td>0.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27550</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27650</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>0.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27750</td>\n",
       "      <td>0.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>0.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27850</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27950</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28050</td>\n",
       "      <td>0.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28150</td>\n",
       "      <td>0.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28250</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>0.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28350</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28450</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28550</td>\n",
       "      <td>0.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28650</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>0.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28750</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28850</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28950</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29050</td>\n",
       "      <td>0.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29150</td>\n",
       "      <td>0.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29250</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29350</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29450</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29550</td>\n",
       "      <td>0.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29650</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29750</td>\n",
       "      <td>0.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>0.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29850</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>0.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29950</td>\n",
       "      <td>0.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30050</td>\n",
       "      <td>0.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>0.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30150</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30250</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>0.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30350</td>\n",
       "      <td>0.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30450</td>\n",
       "      <td>0.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30550</td>\n",
       "      <td>0.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30650</td>\n",
       "      <td>0.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30750</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30850</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>0.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30950</td>\n",
       "      <td>0.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31050</td>\n",
       "      <td>0.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31150</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31250</td>\n",
       "      <td>0.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31350</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>0.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31450</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31550</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31650</td>\n",
       "      <td>0.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31750</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31850</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31950</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32050</td>\n",
       "      <td>0.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32150</td>\n",
       "      <td>0.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>0.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32250</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32350</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32450</td>\n",
       "      <td>0.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32550</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>0.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32650</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32750</td>\n",
       "      <td>0.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32850</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32950</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33050</td>\n",
       "      <td>0.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>0.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33150</td>\n",
       "      <td>0.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33250</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>0.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33350</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33450</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33550</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33650</td>\n",
       "      <td>0.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>0.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33750</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>0.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33850</td>\n",
       "      <td>0.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33950</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34050</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34150</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34250</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34350</td>\n",
       "      <td>0.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34450</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34550</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34650</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>0.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34750</td>\n",
       "      <td>0.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34850</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34950</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35050</td>\n",
       "      <td>0.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>0.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35150</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35250</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>0.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35350</td>\n",
       "      <td>0.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35450</td>\n",
       "      <td>0.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35550</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>0.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35650</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35750</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35850</td>\n",
       "      <td>0.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35950</td>\n",
       "      <td>0.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36050</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36150</td>\n",
       "      <td>0.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36250</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>0.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36350</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>0.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36450</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36550</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36650</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36750</td>\n",
       "      <td>0.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>0.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36850</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36950</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37050</td>\n",
       "      <td>0.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37150</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>0.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37250</td>\n",
       "      <td>0.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37300</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37350</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>0.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37450</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37550</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37650</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37700</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37750</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>0.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37850</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37900</td>\n",
       "      <td>0.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37950</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38050</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38150</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38250</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38300</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38350</td>\n",
       "      <td>0.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38450</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38550</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38650</td>\n",
       "      <td>0.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38750</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38850</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38900</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38950</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39050</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39100</td>\n",
       "      <td>0.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39150</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39250</td>\n",
       "      <td>0.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39300</td>\n",
       "      <td>0.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39350</td>\n",
       "      <td>0.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39450</td>\n",
       "      <td>0.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39550</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39650</td>\n",
       "      <td>0.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39700</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39750</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39850</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39900</td>\n",
       "      <td>0.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39950</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40050</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40100</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40150</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40250</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40300</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40350</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40450</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40550</td>\n",
       "      <td>0.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>0.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40650</td>\n",
       "      <td>0.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40700</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40750</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40850</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40900</td>\n",
       "      <td>0.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40950</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41050</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41100</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41150</td>\n",
       "      <td>0.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>0.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41250</td>\n",
       "      <td>0.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41300</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41350</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41450</td>\n",
       "      <td>0.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41550</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41650</td>\n",
       "      <td>0.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41700</td>\n",
       "      <td>0.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41750</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41850</td>\n",
       "      <td>0.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41900</td>\n",
       "      <td>0.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41950</td>\n",
       "      <td>0.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42050</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42100</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42150</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42250</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42300</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42350</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>0.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42450</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42550</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42650</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42700</td>\n",
       "      <td>0.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42750</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42850</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42900</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42950</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43050</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43100</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43150</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43250</td>\n",
       "      <td>0.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43300</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43350</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43400</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43450</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43550</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43600</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43650</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43700</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43750</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43850</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43900</td>\n",
       "      <td>0.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43950</td>\n",
       "      <td>0.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44050</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44100</td>\n",
       "      <td>0.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44150</td>\n",
       "      <td>0.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44200</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44250</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44300</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44350</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44450</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44550</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44600</td>\n",
       "      <td>0.104200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44650</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44700</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44750</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44850</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44900</td>\n",
       "      <td>0.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44950</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45050</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45100</td>\n",
       "      <td>0.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45150</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45200</td>\n",
       "      <td>0.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45250</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45300</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45350</td>\n",
       "      <td>0.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45400</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45450</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45550</td>\n",
       "      <td>0.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45650</td>\n",
       "      <td>0.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45700</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45750</td>\n",
       "      <td>0.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45800</td>\n",
       "      <td>0.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45850</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45900</td>\n",
       "      <td>0.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45950</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46050</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46100</td>\n",
       "      <td>0.118600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46150</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>0.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46250</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46300</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46350</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46450</td>\n",
       "      <td>0.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46550</td>\n",
       "      <td>0.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46600</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46650</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46700</td>\n",
       "      <td>0.091300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HB] step=500/46713 | epoch=0.03\n",
      "[HB] step=1000/46713 | epoch=0.06\n",
      "[HB] step=1500/46713 | epoch=0.10\n",
      "[HB] step=2000/46713 | epoch=0.13\n",
      "[HB] step=2500/46713 | epoch=0.16\n",
      "[HB] step=3000/46713 | epoch=0.19\n",
      "[HB] step=3500/46713 | epoch=0.22\n",
      "[HB] step=4000/46713 | epoch=0.26\n",
      "[HB] step=4500/46713 | epoch=0.29\n",
      "[HB] step=5000/46713 | epoch=0.32\n",
      "[HB] step=5500/46713 | epoch=0.35\n",
      "[HB] step=6000/46713 | epoch=0.39\n",
      "[HB] step=6500/46713 | epoch=0.42\n",
      "[HB] step=7000/46713 | epoch=0.45\n",
      "[HB] step=7500/46713 | epoch=0.48\n",
      "[HB] step=8000/46713 | epoch=0.51\n",
      "[HB] step=8500/46713 | epoch=0.55\n",
      "[HB] step=9000/46713 | epoch=0.58\n",
      "[HB] step=9500/46713 | epoch=0.61\n",
      "[HB] step=10000/46713 | epoch=0.64\n",
      "[HB] step=10500/46713 | epoch=0.67\n",
      "[HB] step=11000/46713 | epoch=0.71\n",
      "[HB] step=11500/46713 | epoch=0.74\n",
      "[HB] step=12000/46713 | epoch=0.77\n",
      "[HB] step=12500/46713 | epoch=0.80\n",
      "[HB] step=13000/46713 | epoch=0.83\n",
      "[HB] step=13500/46713 | epoch=0.87\n",
      "[HB] step=14000/46713 | epoch=0.90\n",
      "[HB] step=14500/46713 | epoch=0.93\n",
      "[HB] step=15000/46713 | epoch=0.96\n",
      "[HB] step=15500/46713 | epoch=1.00\n",
      "[HB] step=16000/46713 | epoch=1.03\n",
      "[HB] step=16500/46713 | epoch=1.06\n",
      "[HB] step=17000/46713 | epoch=1.09\n",
      "[HB] step=17500/46713 | epoch=1.12\n",
      "[HB] step=18000/46713 | epoch=1.16\n",
      "[HB] step=18500/46713 | epoch=1.19\n",
      "[HB] step=19000/46713 | epoch=1.22\n",
      "[HB] step=19500/46713 | epoch=1.25\n",
      "[HB] step=20000/46713 | epoch=1.28\n",
      "[HB] step=20500/46713 | epoch=1.32\n",
      "[HB] step=21000/46713 | epoch=1.35\n",
      "[HB] step=21500/46713 | epoch=1.38\n",
      "[HB] step=22000/46713 | epoch=1.41\n",
      "[HB] step=22500/46713 | epoch=1.44\n",
      "[HB] step=23000/46713 | epoch=1.48\n",
      "[HB] step=23500/46713 | epoch=1.51\n",
      "[HB] step=24000/46713 | epoch=1.54\n",
      "[HB] step=24500/46713 | epoch=1.57\n",
      "[HB] step=25000/46713 | epoch=1.61\n",
      "[HB] step=25500/46713 | epoch=1.64\n",
      "[HB] step=26000/46713 | epoch=1.67\n",
      "[HB] step=26500/46713 | epoch=1.70\n",
      "[HB] step=27000/46713 | epoch=1.73\n",
      "[HB] step=27500/46713 | epoch=1.77\n",
      "[HB] step=28000/46713 | epoch=1.80\n",
      "[HB] step=28500/46713 | epoch=1.83\n",
      "[HB] step=29000/46713 | epoch=1.86\n",
      "[HB] step=29500/46713 | epoch=1.89\n",
      "[HB] step=30000/46713 | epoch=1.93\n",
      "[HB] step=30500/46713 | epoch=1.96\n",
      "[HB] step=31000/46713 | epoch=1.99\n",
      "[HB] step=31500/46713 | epoch=2.02\n",
      "[HB] step=32000/46713 | epoch=2.06\n",
      "[HB] step=32500/46713 | epoch=2.09\n",
      "[HB] step=33000/46713 | epoch=2.12\n",
      "[HB] step=33500/46713 | epoch=2.15\n",
      "[HB] step=34000/46713 | epoch=2.18\n",
      "[HB] step=34500/46713 | epoch=2.22\n",
      "[HB] step=35000/46713 | epoch=2.25\n",
      "[HB] step=35500/46713 | epoch=2.28\n",
      "[HB] step=36000/46713 | epoch=2.31\n",
      "[HB] step=36500/46713 | epoch=2.34\n",
      "[HB] step=37000/46713 | epoch=2.38\n",
      "[HB] step=37500/46713 | epoch=2.41\n",
      "[HB] step=38000/46713 | epoch=2.44\n",
      "[HB] step=38500/46713 | epoch=2.47\n",
      "[HB] step=39000/46713 | epoch=2.50\n",
      "[HB] step=39500/46713 | epoch=2.54\n",
      "[HB] step=40000/46713 | epoch=2.57\n",
      "[HB] step=40500/46713 | epoch=2.60\n",
      "[HB] step=41000/46713 | epoch=2.63\n",
      "[HB] step=41500/46713 | epoch=2.67\n",
      "[HB] step=42000/46713 | epoch=2.70\n",
      "[HB] step=42500/46713 | epoch=2.73\n",
      "[HB] step=43000/46713 | epoch=2.76\n",
      "[HB] step=43500/46713 | epoch=2.79\n",
      "[HB] step=44000/46713 | epoch=2.83\n",
      "[HB] step=44500/46713 | epoch=2.86\n",
      "[HB] step=45000/46713 | epoch=2.89\n",
      "[HB] step=45500/46713 | epoch=2.92\n",
      "[HB] step=46000/46713 | epoch=2.95\n",
      "[HB] step=46500/46713 | epoch=2.99\n",
      "âœ… Saved to: finetuned_model_3_epoch\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "# ====== Heartbeat callback (in nhá»‹p tim má»—i 500 bÆ°á»›c) ======\n",
    "class Heartbeat(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 500 == 0:\n",
    "            print(f\"[HB] step={state.global_step}/{state.max_steps} | epoch={state.epoch:.2f}\")\n",
    "\n",
    "trainer.add_callback(Heartbeat())   # âœ… gáº¯n callback vÃ o trainer\n",
    "\n",
    "# ====== Resume tá»« checkpoint gáº§n nháº¥t ======\n",
    "ckpts = sorted(Path(output_dir).glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
    "resume_ckpt = str(ckpts[-1]) if ckpts else None\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if resume_ckpt:\n",
    "    print(\"Resuming from:\", resume_ckpt)\n",
    "    trainer.train(resume_from_checkpoint=resume_ckpt)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "    trainer.train()\n",
    "\n",
    "# ====== SAVE ======\n",
    "finetuned_model.save_pretrained(\"finetuned_model_3_epoch\")\n",
    "tokenizer.save_pretrained(\"finetuned_model_3_epoch\")\n",
    "print(\"âœ… Saved to: finetuned_model_3_epoch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33393b",
   "metadata": {},
   "source": [
    "# TRAIN 1EP - 3.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cá»‘ gáº¯ng náº¡p model Ä‘Ã£ fine-tune náº¿u cÃ³ =====\n",
    "try:\n",
    "    # [NOTE] Äá»”I TÃŠN THÆ¯ Má»¤C: dÃ¹ng epoch 3 hiá»‡n cÃ³ trong /kaggle/working\n",
    "    finetuned_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        \"/kaggle/working/finetuned_model_3_epoch\"   # [NOTE]\n",
    "    ).to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\n",
    "        \"/kaggle/working/finetuned_model_3_epoch\"   # [NOTE]\n",
    "    )\n",
    "\n",
    "    to_train = True   # [NOTE] Ä‘á»ƒ tiáº¿p tá»¥c huáº¥n luyá»‡n thÃªm 1 epoch\n",
    "except Exception as e:\n",
    "    to_train = True\n",
    "    if device == \"cuda\":\n",
    "        finetuned_model = T5ForConditionalGeneration.from_pretrained(\n",
    "            base_source, torch_dtype=torch.bfloat16\n",
    "        ).to(device)\n",
    "    else:\n",
    "        finetuned_model = T5ForConditionalGeneration.from_pretrained(base_source).to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(base_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Runtime sanity check ====\n",
      "Train size: 124561\n",
      "per_device_train_batch_size: 4\n",
      "gradient_accumulation_steps: 4\n",
      "len(train_dataloader): 15571\n",
      "=> expected optimizer steps/epoch: 3893\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3893' max='3893' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3893/3893 3:24:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.200200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned_model_4_epoch/tokenizer_config.json',\n",
       " 'finetuned_model_4_epoch/special_tokens_map.json',\n",
       " 'finetuned_model_4_epoch/spiece.model',\n",
       " 'finetuned_model_4_epoch/added_tokens.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, time, torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "finetuned_model.config.use_cache = False\n",
    "finetuned_model.gradient_checkpointing_enable()  # nhÆ° cÅ©\n",
    "\n",
    "if to_train:\n",
    "    output_dir = f\"./sql-training-{int(time.time())}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=5e-3,\n",
    "        num_train_epochs=1,                 # [NOTE] chá»‰ thÃªm Ä‘Ãºng 1 epoch\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,\n",
    "        optim=\"adafactor\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=finetuned_model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "    )\n",
    "\n",
    "    # (Tuá»³ chá»n) sanity check trÆ°á»›c khi train\n",
    "    import math\n",
    "    dl = trainer.get_train_dataloader()\n",
    "    print(\"==== Runtime sanity check ====\")\n",
    "    print(\"Train size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"per_device_train_batch_size:\", training_args.per_device_train_batch_size)\n",
    "    print(\"gradient_accumulation_steps:\", training_args.gradient_accumulation_steps)\n",
    "    print(\"len(train_dataloader):\", len(dl))\n",
    "    steps_per_epoch = math.ceil(len(dl) / training_args.gradient_accumulation_steps)\n",
    "    print(\"=> expected optimizer steps/epoch:\", steps_per_epoch)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # [NOTE] KHÃ”NG truyá»n resume_from_checkpoint â€” vÃ¬ Ä‘Ã£ load trá»ng sá»‘ epoch-3 vÃ o model rá»“i\n",
    "    trainer.train()\n",
    "    \n",
    "print(\"âœ… Training completed successfully!\")\n",
    "finetuned_model.save_pretrained(\"finetuned_model_4_epoch\")   # [NOTE] Ä‘á»•i tÃªn thÆ° má»¥c lÆ°u\n",
    "tokenizer.save_pretrained(\"finetuned_model_4_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e64d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Äang nÃ©n model: /kaggle/working/finetuned_model_4_epoch\n",
      "âœ… ÄÃ£ nÃ©n model thÃ nh cÃ´ng!\n",
      "ğŸ“¦ File nÃ©n náº±m táº¡i: /kaggle/working/finetuned_model_ï¼”_epoch_backup.zip\n",
      "ğŸ’¾ Dung lÆ°á»£ng: 214.56 MB\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "\n",
    "#  ThÆ° má»¥c model cáº§n nÃ©n\n",
    "model_dir = \"/kaggle/working/finetuned_model_4_epoch\"\n",
    "\n",
    "#  ÄÆ°á»ng dáº«n file zip Ä‘áº§u ra (khÃ´ng thÃªm .zip á»Ÿ Ä‘Ã¢y)\n",
    "output_zip = f\"/kaggle/working/finetuned_model_ï¼”_epoch_backup\"\n",
    "\n",
    "#  Tiáº¿n hÃ nh nÃ©n riÃªng thÆ° má»¥c model\n",
    "print(\"ğŸ”„ Äang nÃ©n model:\", model_dir)\n",
    "shutil.make_archive(output_zip, 'zip', model_dir)\n",
    "\n",
    "#  Kiá»ƒm tra káº¿t quáº£\n",
    "zip_path = f\"{output_zip}.zip\"\n",
    "if os.path.exists(zip_path):\n",
    "    size_mb = os.path.getsize(zip_path) / (1024**2)\n",
    "    print(\"âœ… ÄÃ£ nÃ©n model thÃ nh cÃ´ng!\")\n",
    "    print(f\"ğŸ“¦ File nÃ©n náº±m táº¡i: {zip_path}\")\n",
    "    print(f\"ğŸ’¾ Dung lÆ°á»£ng: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"âš ï¸ CÃ³ lá»—i xáº£y ra: KhÃ´ng tÃ¬m tháº¥y file nÃ©n sau khi táº¡o.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
